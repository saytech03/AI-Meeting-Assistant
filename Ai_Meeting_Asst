{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Check if GPU is available\nimport torch\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nprint(f\"GPU name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:39:04.267429Z","iopub.execute_input":"2025-08-28T19:39:04.267677Z","iopub.status.idle":"2025-08-28T19:39:08.617108Z","shell.execute_reply.started":"2025-08-28T19:39:04.267655Z","shell.execute_reply":"2025-08-28T19:39:08.616464Z"}},"outputs":[{"name":"stdout","text":"GPU available: True\nGPU name: Tesla T4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Install necessary packages (ignore the dependency warnings)\n!pip install transformers torchaudio datasets soundfile openai-whisper gradio --quiet\nprint(\"Packages installed successfully! Ignore any dependency warnings above.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:37:34.272411Z","iopub.execute_input":"2025-08-28T19:37:34.272594Z","iopub.status.idle":"2025-08-28T19:39:04.265743Z","shell.execute_reply.started":"2025-08-28T19:37:34.272576Z","shell.execute_reply":"2025-08-28T19:39:04.264879Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mPackages installed successfully! Ignore any dependency warnings above.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport os\nimport tempfile\nimport subprocess\nimport whisper\nfrom transformers import pipeline\nimport gradio as gr\nimport re\nfrom datetime import datetime\n\nprint(\"All libraries imported successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:51:04.867449Z","iopub.execute_input":"2025-08-28T19:51:04.867773Z","iopub.status.idle":"2025-08-28T19:51:41.813190Z","shell.execute_reply.started":"2025-08-28T19:51:04.867737Z","shell.execute_reply":"2025-08-28T19:51:41.812531Z"}},"outputs":[{"name":"stderr","text":"2025-08-28 19:51:18.538349: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756410678.900097      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756410678.998597      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"All libraries imported successfully!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load the Whisper model onto the GPU (if available)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Load models\nmodel = whisper.load_model(\"small\").to(device)\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0 if device == \"cuda\" else -1)\n\nprint(\"AI models loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:52:17.835856Z","iopub.execute_input":"2025-08-28T19:52:17.836815Z","iopub.status.idle":"2025-08-28T19:52:49.638266Z","shell.execute_reply.started":"2025-08-28T19:52:17.836790Z","shell.execute_reply":"2025-08-28T19:52:49.637625Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 461M/461M [00:16<00:00, 29.5MiB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b5ef20da7dc4dec9c4aaf25408504be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d86b145cd804c0b87b4d2ce2efb9568"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b2f1c4ad7bb46cfbd082b00ccaa324f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"072c2a6163264c8e99740ec3ea15baca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1924ef82dcf84db0af96966d415e3611"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1fcd9d7dd094e0e809e1a55d95464ae"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"AI models loaded successfully!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def convert_audio_to_wav(audio_path):\n    \"\"\"Convert any audio file to WAV format that Whisper can handle\"\"\"\n    try:\n        # Create a temporary WAV file\n        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_wav:\n            temp_wav_path = temp_wav.name\n        \n        # Use ffmpeg to convert to WAV (16kHz, 16-bit, mono) - Whisper's preferred format\n        command = [\n            'ffmpeg', '-i', audio_path, '-ac', '1', '-ar', '16000', \n            '-acodec', 'pcm_s16le', '-y', temp_wav_path\n        ]\n        \n        # Run the conversion (suppress output)\n        subprocess.run(command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n        \n        return temp_wav_path\n    \n    except Exception as e:\n        print(f\"Audio conversion error: {e}\")\n        return audio_path  # Fall back to original file\n\ndef extract_action_items(text):\n    \"\"\"Extract potential action items from text\"\"\"\n    patterns = [\n        r\"we need to ([^\\.]*)\",\n        r\"let's ([^\\.]*)\", \n        r\"action item:? ([^\\.]*)\",\n        r\"todo:? ([^\\.]*)\",\n        r\"will ([^\\.]*)\",\n        r\"going to ([^\\.]*)\",\n        r\"please ([^\\.]*)\",\n        r\"make sure to ([^\\.]*)\"\n    ]\n    \n    action_items = []\n    for pattern in patterns:\n        matches = re.findall(pattern, text, re.IGNORECASE)\n        action_items.extend(matches)\n    \n    return list(set(action_items))[:5]\n\ndef process_meeting(audio_path):\n    \"\"\"Process audio and generate meeting insights with error handling\"\"\"\n    if audio_path is None:\n        return \"No audio detected\", \"\", \"\"\n    \n    try:\n        # Convert audio to compatible format first\n        converted_audio_path = convert_audio_to_wav(audio_path)\n        \n        # Transcribe the audio with better parameters\n        result = model.transcribe(\n            converted_audio_path,\n            fp16=torch.cuda.is_available(),  # Use GPU precision if available\n            language='en',  # Specify English for better accuracy\n            task='transcribe'\n        )\n        \n        transcription = result[\"text\"]\n        \n        # Clean up temporary file (THIS IS WHERE THE ERROR WAS)\n        if converted_audio_path != audio_path:\n            os.unlink(converted_audio_path)\n        \n        # Handle empty or very short transcriptions\n        if not transcription.strip() or len(transcription.split()) < 3:\n            return \"Could not transcribe audio. Please try again with clearer speech.\", \"\", \"\"\n        \n        # Generate summary\n        if len(transcription.split()) < 10:\n            summary = \"Speech too short to generate a meaningful summary.\"\n        else:\n            summary_result = summarizer(transcription, max_length=150, min_length=30, do_sample=False)\n            summary = summary_result[0]['summary_text']\n        \n        # Extract action items\n        action_items = extract_action_items(transcription)\n        action_items_str = \"\\n\".join([f\"{i+1}. {item.capitalize()}\" for i, item in enumerate(action_items)]) if action_items else \"No clear action items identified.\"\n        \n        return transcription, summary, action_items_str\n        \n    except Exception as e:\n        return f\"Error processing audio: {str(e)}\", \"\", \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:56:23.692430Z","iopub.execute_input":"2025-08-28T19:56:23.693070Z","iopub.status.idle":"2025-08-28T19:56:23.702011Z","shell.execute_reply.started":"2025-08-28T19:56:23.693045Z","shell.execute_reply":"2025-08-28T19:56:23.701403Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# First install ffmpeg for audio conversion\n!apt-get update && apt-get install -y ffmpeg\n\n# Create the Gradio interface with better error handling\niface = gr.Interface(\n    fn=process_meeting,\n    inputs=gr.Audio(sources=\"microphone\", type=\"filepath\", label=\"Record your meeting\"),\n    outputs=[\n        gr.Textbox(label=\"Transcription\", lines=6),\n        gr.Textbox(label=\"Summary\", lines=4), \n        gr.Textbox(label=\"Action Items\", lines=4)\n    ],\n    title=\"ğŸ™ï¸ AI Meeting Co-Pilot\",\n    description=\"\"\"Record your meeting (speak clearly for 30+ seconds) and get:\n    â€¢ Real-time transcription\n    â€¢ AI-generated summary  \n    â€¢ Action items extracted\n    \"\"\",\n    allow_flagging=\"never\"\n)\n\n# Launch with better configuration\nprint(\"ğŸš€ Launching Meeting Co-Pilot...\")\niface.launch(share=True, debug=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T19:57:05.517029Z","iopub.execute_input":"2025-08-28T19:57:05.517275Z","iopub.status.idle":"2025-08-28T19:57:13.992622Z","shell.execute_reply.started":"2025-08-28T19:57:05.517258Z","shell.execute_reply":"2025-08-28T19:57:13.991980Z"}},"outputs":[{"name":"stdout","text":"Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\nGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\nGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\nGet:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]           \nHit:5 http://archive.ubuntu.com/ubuntu jammy InRelease                         \nGet:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \nGet:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,940 kB]\nGet:8 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,272 kB]\nGet:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,340 kB]\nGet:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,782 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]     \nGet:12 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [48.5 kB]\nGet:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,297 kB]\nGet:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,199 kB] \nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,608 kB]\nGet:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\nGet:17 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\nHit:18 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease   \nGet:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.2 kB]\nGet:20 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [36.0 kB]\nGet:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,526 kB]\nGet:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,576 kB]\nGet:23 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [75.9 kB]\nGet:24 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\nGet:25 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\nFetched 35.3 MB in 3s (13.8 MB/s)                            \nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n0 upgraded, 0 newly installed, 0 to remove and 138 not upgraded.\nğŸš€ Launching Meeting Co-Pilot...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/gradio/interface.py:416: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://6d81fa7294476d860a.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://6d81fa7294476d860a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"gradio deploy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T20:23:11.195282Z","iopub.execute_input":"2025-08-28T20:23:11.195633Z","iopub.status.idle":"2025-08-28T20:23:11.201672Z","shell.execute_reply.started":"2025-08-28T20:23:11.195608Z","shell.execute_reply":"2025-08-28T20:23:11.200621Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_36/316834953.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    gradio deploy\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (316834953.py, line 1)","output_type":"error"}],"execution_count":8}]}